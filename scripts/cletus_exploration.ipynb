{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the tweets data\n",
    "tweets_df = pd.read_csv(\"Tweets_source_data.csv\")\n",
    "# \n",
    "len(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displays tweets share by airlines\n",
    "tweets_df.airline.value_counts().plot(kind='pie', autopct='%1.0f%%', radius=1.45, label='',shadow=True)\n",
    "plt.savefig('../images/airlinestweets.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display count of the different sentiments - negative, neutral, positive\n",
    "tweets_df.airline_sentiment.value_counts().plot(kind='pie', autopct='%1.0f%%', radius=1.48, label='',shadow=True)\n",
    "plt.savefig('../images/tweetsenti.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#represent sentiments by airlines on a bar chart\n",
    "tweets_df.groupby(['airline', 'airline_sentiment']).airline_sentiment.count().unstack().plot(kind='bar', figsize=(12,7), legend=True, title='Tweets by Airlines')\n",
    "plt.savefig('../images/tweets_by_sent.png')\n",
    "#plt.legend(loc='best')\n",
    "# sentiment_airline.plot(kind='bar', loc='best')\n",
    "\n",
    "# sns.catplot(x=\"airline\", y=\"countval\", hue=\"airline_sentiment\", data=sentiment_airline,\n",
    "#             height=6, kind=\"bar\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "- Looking through the different tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#processing each individual tweet, by removing multiple spaces, single character, airline name\n",
    "tweets = tweets_df['text']\n",
    "tweets\n",
    "\n",
    "processed_tweets = []\n",
    "\n",
    "for line in range(0, len(tweets)):\n",
    "#     proc_line = tweets[line].split()[1:]\n",
    "    # Remove all the special characters\n",
    "    proc_line = re.sub(r'\\W', ' ', str(tweets[line]))\n",
    "\n",
    "    # remove all single characters\n",
    "    proc_line= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', proc_line)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    proc_line = re.sub(r'\\^[a-zA-Z]\\s+', ' ', proc_line) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    proc_line = re.sub(r'\\s+', ' ', proc_line, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    proc_line = re.sub(r'^b\\s+', '', proc_line)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    proc_line = proc_line.lower()\n",
    "    \n",
    "    # remove the name of the airline\n",
    "    proc_line = ' '.join(proc_line.split()[1:])\n",
    "    \n",
    "    # add to the created list\n",
    "    processed_tweets.append(proc_line)\n",
    "    \n",
    "\n",
    "#add a new column to the df with the clean tweet\n",
    "tweets_df['tweets'] = processed_tweets\n",
    "#tweets_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a word cloud of the most popular word in the tweets.\n",
    "def wordcloud(tweets,col):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(background_color='white', colormap=\"gnuplot\", contour_color='steelblue', max_words=2000, stopwords=stopwords).generate(\" \".join([i for i in tweets[col]]))\n",
    "    plt.figure( figsize=(20,10), facecolor='k')\n",
    "    plt.imshow(wordcloud, cmap=plt.cm.gray, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('../images/wordcloud.png')\n",
    "wordcloud(tweets_df,'tweets')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Model\n",
    "- We will classify and use only the top 5000 words that occur in 80% of the sentences.\n",
    "- We will use random forest classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a vectorizer to be used for classification\n",
    "vectorizer = TfidfVectorizer (max_features=5000, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
    "processed_tweets = vectorizer.fit_transform(processed_tweets).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting our train and test data\n",
    "airlines = tweets_df['airline']\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_tweets, airlines, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train our model\n",
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction\n",
    "predictions = text_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[233  32  16  73 193   0]\n",
      " [ 27 189  40  53 150   2]\n",
      " [ 51  37 160  59 179   2]\n",
      " [ 80  21  27 260 212   0]\n",
      " [ 58  36  47  81 497   2]\n",
      " [ 12  17  15  10  48   9]]\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      American       0.51      0.43      0.46       547\n",
      "         Delta       0.57      0.41      0.48       461\n",
      "     Southwest       0.52      0.33      0.40       488\n",
      "    US Airways       0.49      0.43      0.46       600\n",
      "        United       0.39      0.69      0.50       721\n",
      "Virgin America       0.60      0.08      0.14       111\n",
      "\n",
      "      accuracy                           0.46      2928\n",
      "     macro avg       0.51      0.39      0.41      2928\n",
      "  weighted avg       0.49      0.46      0.45      2928\n",
      "\n",
      "0.4603825136612022\n"
     ]
    }
   ],
   "source": [
    "#generating reports\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
